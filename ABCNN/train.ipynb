{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from load_data.ipynb\n",
      "importing Jupyter notebook from G:\\GitHub\\jupyter_test\\ABCNN\\abcnn\\basic.ipynb\n",
      "importing Jupyter notebook from G:\\GitHub\\jupyter_test\\ABCNN\\abcnn\\attentive_pooling.ipynb\n",
      "importing Jupyter notebook from G:\\GitHub\\jupyter_test\\ABCNN\\abcnn\\abcnn.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import import_ipynb\n",
    "from load_data import QA_Preprocessor\n",
    "\n",
    "from abcnn import Basic_AP, Attentive_Pooling, Attentive_Pooling_1d, Attentive_Pooling_2d, ABCNN\n",
    "\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='ABCNN-implementation in pytorch')\n",
    "parser.add_argument('--cuda', action='store_true',\n",
    "                    help='use CUDA (default: False)')\n",
    "parser.add_argument('--eval', action='store_true',\n",
    "                    help='do evaluate (default: False)')\n",
    "parser.add_argument('--dropout', type=float, default=0,\n",
    "                    help='dropout applied to layers (default: 0)')\n",
    "parser.add_argument('--clip', type=float, default=-1,\n",
    "                    help='gradient clip, -1 means no clip (default: -1)')\n",
    "parser.add_argument('--epochs', type=int, default=15,\n",
    "                    help='upper epoch limit (default: 500)')\n",
    "parser.add_argument('--report_step', type=int, default=1, metavar='N',\n",
    "                    help='report interval (default: 20')\n",
    "parser.add_argument('--valid_step', type=int, default=1, metavar='N',\n",
    "                    help='validation interval (default: 20')\n",
    "parser.add_argument('--lr', type=float, default=0.001,\n",
    "                    help='initial learning rate (default: 1e-3)')\n",
    "parser.add_argument('--optim', type=str, default='Adam',\n",
    "                    help='optimizer to use (default: Adam)')\n",
    "parser.add_argument('--batchs', type=int, default=128,\n",
    "                    help='number of batchs (default: 10)')\n",
    "parser.add_argument('--val_batchs', type=int, default=32,\n",
    "                    help='number of batchs (default: 10)')\n",
    "parser.add_argument('--train_data', type=str, default='data/WikiQACorpus/WikiQA.train',\n",
    "                    help='the train dataset to run (default: data/WikiQACorpus/WikiQA-train.txt)')\n",
    "parser.add_argument('--dev_data', type=str, default='data/WikiQACorpus/WikiQA.dev',\n",
    "                    help='the dataset to run')\n",
    "parser.add_argument('--test_data', type=str, default='data/WikiQACorpus/WikiQA.test',\n",
    "                    help='the dataset to run (default: WikiQA)')\n",
    "parser.add_argument('--seed', type=int, default=190330,\n",
    "                    help='random seed (default: 190330)')\n",
    "parser.add_argument('--model_name', type=str, default='AP',\n",
    "                    help='the dataset to run (default: )')\n",
    "parser.add_argument('--filter_width', type=int, default=3,\n",
    "                    help='width of all filters (default: 4)')\n",
    "parser.add_argument('--num_layer', type=int, default=1,\n",
    "                    help='number of layers (default: 1)')\n",
    "parser.add_argument('--embedding_size', type=int, default=300,\n",
    "                    help='dimension of embeddings (default: 5)')\n",
    "parser.add_argument('--max_length', type=int, default=40,\n",
    "                    help='maximum of tokens for each sentence (default: 40)')\n",
    "\n",
    "args = parser.parse_args(['--cuda', '--eval', '--model_name', 'AP2d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batchs=128, clip=-1, cuda=True, dev_data='data/WikiQACorpus/WikiQA.dev', dropout=0, embedding_size=300, epochs=15, eval=True, filter_width=3, lr=0.001, max_length=40, model_name='AP2d', num_layer=1, optim='Adam', report_step=1, seed=190330, test_data='data/WikiQACorpus/WikiQA.test', train_data='data/WikiQACorpus/WikiQA.train', val_batchs=32, valid_step=1)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: Please use a CUDA device. Run with --cuda\")\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-04-02 22:11:42.197894] Data_load done. Max token size: 409\n",
      "\tVocabulary size: 8869\n"
     ]
    }
   ],
   "source": [
    "data_preprocessor = QA_Preprocessor(args.train_data, length_limit=args.max_length)\n",
    "data = data_preprocessor.data\n",
    "Q_train = torch.tensor(data['q'], dtype=torch.long)\n",
    "A_train = torch.tensor(data['a'], dtype=torch.long)\n",
    "Y_train = torch.tensor(data['y'], dtype=torch.float)\n",
    "assert Q_train.shape[0] == A_train.shape[0] == Y_train.shape[0]\n",
    "#embeds = nn.Embedding(len_vocab, 5, padding_idx=0)\n",
    "#hello_embed = embeds(Q_train)\n",
    "\n",
    "data_preprocessor.reset_data()\n",
    "data_preprocessor.load_raw_file(args.dev_data)\n",
    "data = data_preprocessor.data\n",
    "Q_dev = torch.tensor(data['q'], dtype=torch.long)\n",
    "A_dev = torch.tensor(data['a'], dtype=torch.long)\n",
    "Y_dev = torch.tensor(data['y'], dtype=torch.float)\n",
    "\n",
    "Q_test = None\n",
    "A_test = None\n",
    "Y_test = None\n",
    "if args.eval:\n",
    "    data_preprocessor.reset_data()\n",
    "    data_preprocessor.load_raw_eval_file(args.test_data)\n",
    "    data = data_preprocessor.data\n",
    "    Q_test = data['q']\n",
    "    A_test = data['a']\n",
    "    Y_test = data['y']\n",
    "\n",
    "vocab_size = len(data_preprocessor.vocab)\n",
    "print('\\tVocabulary size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "if args.model_name == 'Basic_AP':\n",
    "    model = Basic_AP(vocab_size, args.embedding_size, args.max_length)\n",
    "elif args.model_name == 'AP':\n",
    "    model = Attentive_Pooling(vocab_size, args.embedding_size, args.max_length, kernel_size=args.filter_width)\n",
    "elif args.model_name == 'AP1d':\n",
    "    model = Attentive_Pooling_1d(vocab_size, args.embedding_size, args.max_length, \n",
    "                                 kernel_size=args.filter_width)\n",
    "elif args.model_name == 'AP2d':\n",
    "    model = Attentive_Pooling_2d(vocab_size, args.embedding_size, args.max_length, \n",
    "                                 kernel_size=args.filter_width)\n",
    "elif args.model_name == 'ABCNN':\n",
    "    model = ABCNN(vocab_size, args.embedding_size, args.max_length, \n",
    "                                 kernel_size=args.filter_width)\n",
    "else:\n",
    "    print('No such model name')\n",
    "    exit()\n",
    "    \n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = None\n",
    "if args.optim == 'Adam' or args.optim == 'RMSprop':\n",
    "    optimizer = getattr(optim, args.optim)(model.parameters(), lr=args.lr)\n",
    "    \n",
    "#criterion = nn.CosineEmbeddingLoss(margin=0.1)\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "#criterion = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for parameter in model.parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_q = Q_train[:2].cuda()\n",
    "x_a = A_train[:2].cuda()\n",
    "model.forward(x_q, x_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_index(total_length, batch_size, do_shuffle = True):\n",
    "    train_idx_list = np.arange(total_length)\n",
    "    if do_shuffle:\n",
    "        np.random.shuffle(train_idx_list)\n",
    "    batch_indices = []\n",
    "    for i in range((total_length // batch_size)+1):\n",
    "        start_idx = i*batch_size\n",
    "        end_idx = min(total_length, (i+1)*batch_size)\n",
    "        if start_idx == end_idx:\n",
    "            break\n",
    "        sub_indices = train_idx_list[start_idx : end_idx]\n",
    "        batch_indices.append(sub_indices)\n",
    "    return batch_indices\n",
    "\n",
    "def train_per_epoch(batch_size):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    count = len(Q_train)\n",
    "    batch_indices = generate_batch_index(count, batch_size)\n",
    "    for idx_list in batch_indices:\n",
    "        x_q = Variable(Q_train[idx_list])\n",
    "        x_a = Variable(A_train[idx_list])\n",
    "        y = Variable(Y_train[idx_list])\n",
    "        if args.cuda:\n",
    "            x_q, x_a, y = x_q.cuda(), x_a.cuda(), y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(x_q, x_a)\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        total_loss += loss.item() /count\n",
    "        if args.clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Q_test, A_test, Y_test):\n",
    "    model.eval()\n",
    "    mean_avg_precision = 0\n",
    "    mean_rr = 0\n",
    "    num_q = len(Q_test)\n",
    "    with torch.no_grad():\n",
    "        for test_step in range(num_q):\n",
    "            x_q = torch.tensor(Q_test[test_step], dtype=torch.long)\n",
    "            x_q = Variable(x_q)\n",
    "            x_a = torch.tensor(A_test[test_step], dtype=torch.long)\n",
    "            x_a = Variable(x_a)\n",
    "            y = torch.tensor(Y_test[test_step], dtype=torch.long)\n",
    "            y = Variable(y)\n",
    "            if args.cuda:\n",
    "                x_q, x_a = x_q.cuda(), x_a.cuda()\n",
    "            output = model(x_q, x_a)\n",
    "            output = output.cpu()\n",
    "            if torch.isnan(output):\n",
    "                print('output error on evaluation')\n",
    "                exit()\n",
    "            add_mrr = True\n",
    "            num_rel = 0\n",
    "            avg_pre = 0\n",
    "            for rank, idx in enumerate(np.argsort(-output, axis=0)):                \n",
    "                if y[idx] == 1:\n",
    "                    #MRR\n",
    "                    if add_mrr:\n",
    "                        mean_rr += 1/(rank+1)\n",
    "                        add_mrr = False\n",
    "                    #MAP\n",
    "                    num_rel += 1\n",
    "                    avg_pre += num_rel / (rank+1)\n",
    "            if num_rel > 0:\n",
    "                avg_pre = avg_pre / num_rel\n",
    "            mean_avg_precision += avg_pre\n",
    "    mean_rr = mean_rr / num_q\n",
    "    mean_avg_precision = mean_avg_precision / num_q\n",
    "    return mean_avg_precision, mean_rr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(batch_size):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = len(Q_dev)\n",
    "    batch_indices = generate_batch_index(count, batch_size, do_shuffle = False)\n",
    "    with torch.no_grad():\n",
    "        for idx_list in batch_indices:\n",
    "            x_q = Variable(Q_dev[idx_list])\n",
    "            x_a = Variable(A_dev[idx_list])\n",
    "            y = Variable(Y_dev[idx_list])\n",
    "            if args.cuda:\n",
    "                x_q, x_a, y = x_q.cuda(), x_a.cuda(), y.cuda()\n",
    "            output = model(x_q, x_a)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item() \n",
    "        eval_loss = total_loss / count\n",
    "        return eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-04-02 22:11:44.053312] model: <AP2d>; lr: <0.00100>; optimizer: <Adam>\n",
      "[2019-04-02 22:11:56.797610] Step  1/15; loss: 0.20736; 12.7s \n",
      "\t Validation loss 0.19943 \n",
      "[2019-04-02 22:12:08.947725] Step  2/15; loss: 0.19651; 24.9s \n",
      "\t Validation loss 0.19472 \n",
      "[2019-04-02 22:12:21.131828] Step  3/15; loss: 0.18875; 37.1s \n",
      "\t Validation loss 0.19225 \n",
      "[2019-04-02 22:12:33.307934] Step  4/15; loss: 0.17942; 49.3s \n",
      "\t Validation loss 0.18486 \n",
      "[2019-04-02 22:12:45.495036] Step  5/15; loss: 0.16785; 61.4s \n",
      "\t Validation loss 0.18068 \n",
      "[2019-04-02 22:12:57.672142] Step  6/15; loss: 0.15621; 73.6s \n",
      "\t Validation loss 0.18014 \n",
      "[2019-04-02 22:13:09.892233] Step  7/15; loss: 0.14427; 85.8s \n",
      "\t Validation loss 0.18030 \n",
      "[2019-04-02 22:13:22.151313] Step  8/15; loss: 0.13330; 98.1s \n",
      "\t Validation loss 0.18288 \n",
      "[2019-04-02 22:13:34.296435] Step  9/15; loss: 0.12304; 110.2s \n",
      "\t Validation loss 0.18659 \n",
      "[2019-04-02 22:13:46.581054] Step 10/15; loss: 0.11290; 122.5s \n",
      "\t Validation loss 0.19091 \n",
      "[2019-04-02 22:13:58.977863] Step 11/15; loss: 0.10324; 134.9s \n",
      "\t Validation loss 0.19673 \n",
      "[2019-04-02 22:14:11.282897] Step 12/15; loss: 0.09505; 147.2s \n",
      "\t Validation loss 0.20322 \n",
      "[2019-04-02 22:14:23.395024] Step 13/15; loss: 0.08753; 159.3s \n",
      "\t Validation loss 0.20961 \n",
      "[2019-04-02 22:14:35.493155] Step 14/15; loss: 0.08081; 171.4s \n",
      "\t Validation loss 0.21824 \n",
      "[2019-04-02 22:14:47.644268] Step 15/15; loss: 0.07492; 183.6s \n",
      "\t Validation loss 0.22473 \n",
      "--------------------------------------------------------------------------------\n",
      "Best performance: 0.18014\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'isnan'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4e8cddfdf20e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best performance: {:.5f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_vloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mmAP_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmrr_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Evaluation- MAP: {:.5f}, MRR: {:.5f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmAP_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmrr_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-0984e27ab111>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(Q_test, A_test, Y_test)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output error on evaluation'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'isnan'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    best_vloss = None\n",
    "    model_file = \"./save/{0}.pt\".format(args.model_name)\n",
    "    start_time = datetime.now()\n",
    "    print(\"[{:s}] model: <{:s}>; lr: <{:.5f}>; optimizer: <{:s}>\".format(\n",
    "        str(start_time), args.model_name, args.lr, args.optim))\n",
    "    for ep in range(1, args.epochs+1):\n",
    "        rloss = train_per_epoch(args.batchs)       \n",
    "        \n",
    "        if ep % args.report_step == 0:\n",
    "            now = datetime.now()\n",
    "            dist_time = now - start_time            \n",
    "            print(\"[{:s}] Step {:2d}/{:2d}; loss: {:.5f}; {:.1f}s \".format(str(now), \n",
    "                                                                           ep, \n",
    "                                                                           args.epochs, \n",
    "                                                                           rloss, \n",
    "                                                                           dist_time.total_seconds()))\n",
    "\n",
    "        if ep % args.valid_step == 0:\n",
    "            vloss = validate(args.val_batchs)\n",
    "            print(\"\\t Validation loss {:.5f} \".format(vloss))\n",
    "            if (best_vloss == None) or vloss < best_vloss:\n",
    "                torch.save({'state_dict': model.state_dict()}, model_file)\n",
    "                best_vloss = vloss\n",
    "    print('-' * 80)\n",
    "    checkpoint = torch.load(model_file)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    \n",
    "    best_vloss = validate(args.val_batchs)\n",
    "    print('Best performance: {:.5f}'.format(best_vloss))    \n",
    "    if args.eval:\n",
    "        mAP_score, mrr_score = evaluate(Q_test, A_test, Y_test)\n",
    "        print('Evaluation- MAP: {:.5f}, MRR: {:.5f}'.format(mAP_score, mrr_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
